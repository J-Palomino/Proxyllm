model_list:
  # Ollama models via hugo (Cloudflare Tunnel)
  - model_name: llama3
    litellm_params:
      model: ollama/llama3
      api_base: https://public-processing-face-trainers.trycloudflare.com
    model_info:
      id: "hugo-llama3"

  - model_name: llama2
    litellm_params:
      model: ollama/llama2
      api_base: https://public-processing-face-trainers.trycloudflare.com
    model_info:
      id: "hugo-llama2"

  - model_name: mistral
    litellm_params:
      model: ollama/mistral
      api_base: https://public-processing-face-trainers.trycloudflare.com
    model_info:
      id: "hugo-mistral"

  # Wildcard support for any Ollama model on hugo
  - model_name: "ollama/*"
    litellm_params:
      model: "ollama/*"
      api_base: https://public-processing-face-trainers.trycloudflare.com

litellm_settings:
  drop_params: True
  num_retries: 3
  request_timeout: 600
  telemetry: False

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  store_model_in_db: True  # Enable model management via UI
  database_url: os.environ/DATABASE_URL
  database_connection_pool_limit: 20
  database_connection_timeout: 60

  # IP Whitelist - Add your approved client IPs here
  # allowed_ips:
  #   - "1.2.3.4"      # Example client IP 1
  #   - "5.6.7.8"      # Example client IP 2
  #   - "10.0.0.0/24"  # Example IP range
